---
title: "Datapalooza Report"
author: "Jessica Kain"
date: "11/22/2018"
output: html_document
---

I attended Datapalooza all day and got the opportunity to learn about numerous exciting research projects related to data science. Personally, learning about the wide range of research paths related to data science was the highlight of my experience at Datapalooza because I've been extremely interested in pursuing a more computational career but have always struggled to identify specific paths that interest me within the computer science realm. What I loved most about Datapalooza was how all the presentations were general enough to have important takeaways for everyone at the event, but were given in the context of their specific projects and fields which allowed us to get a feel for the range of applications for these computational techniques. Below I summarize the main events of the day and discuss some of the presentations that were really intriguing to me in a little more detail. 

The first event of the day was the fireside chat with Phil Bourne and Jim Ryan, UVA's new president. Phil Bourne was the Associate Director for Data Science and a Senior Investigator at the National Center for Biotechnology Information at NIH, and was recently hired by UVA as our Director of the Data Science Institute. He is also a professor in our BME department. The chat was a nice way to start the day because it was laid back but also got the crowd very excited about the rest of the events. Our Data Science Institute is relatively new and having an expert like Phil Bourne as the director is extremely exciting. They talked about their plans and goals for the program as well as other related UVA programs. I have recently been interested in the Data Science Master’s Program here and was extremely grateful for the opportunity to hear Dr. Bourne discuss the future of the institute. 

Next were the research highlights in Machine Learning, Data Integration & Engineering, Data Visualization, and Ethics. Unfortunately, these research highlights took place simultaneously so we had to choose one. I chose to attend the research highlights on machine learning, and I am very happy that I did. Four research highlights in Machine Learning were presented during this time: ensemble clustering in python, automatic quantification of cardiac MRI for hypertrophic cardiomyopathy, plagiarism detection using semantic analysis, and teaching autonomous vehicles to handle edge cases in traffic. 

My favorite of these was by far Madhur Behl's presentation on deepracing: teaching autonomous vehicles to handle edge cases in traffic. The reason this presentation stuck out to me the most from the entire event was because I've always been extremely fascinated with autonomous cars and never realized the importance of Machine Learning in its development. Professor Behl's presentation was based off a lesson in ML we've encountered numerous times in our class: machine intelligence is largely, if not all, about the training data. With respect to his research, this explains the challenges of ensuring a car operates appropriately when it does not recognize its environment or aspects of its environment. He showed us a couple examples of real life examples that autonomous cars struggle with. One of which was a commercial sign for a restaurant that looks like a 'do not enter' road sign. How can we ensure that autonomous cars can make little distinctions and not be thrown off by unfamiliar environments? The answer to all of the challenges is to train the machine on more data, but more importantly data that exposes the machine to these challenging scenarios. Professor Behl's presentation was focused on the challenge of handling edges in traffic. His approach to training the machines to learn how to handle edges is by making them race against each other on tracks because race tracks expose cars to the toughest edge cases possible and the most frequently. Not only did this end up being a very fun research tactic for his team, but was actually successful in improving the cars' edge handling. 

I also found Samarth Singh’s presentation on plagiarism detection using semantic analysis to be very interesting. In CS 2150 this semester, we learned a little about their plagiarism detection programs that use semantic analysis, which essentially transforms code into a tree-diagram based off of its structure and compares the tree-diagrams of different code to determine the level of similarity. While I have heard of this technique being used often for catching plagiarized code, I never thought of how the same methodology could be applied to written assignments. Instead of determining structure based off the use of variables, functions, parameters, etc., the structure of writing can be determined by the use of nouns, verbs, adjectives, etc. Again, the key aspect of this process is training the machine with a lot of data, in this case writing styles and examples of plagiarism. The more styles of plagiarism the machine is exposed to, such as sentence reformation and other techniques to use someone else’s writing without directly copying it, the more likely the detection software is to catch plagiarism. 

I registered for Datapalooza when we first got the email, but did not sign up for a skills session because I didn’t know what they were and didn’t think they would get filled so quickly if I decided to go. I did attend the Tech Talks during the skills sessions, however. There were two tech talks: “You have Machine Learning. Now what?” by Miriam Friedel from Metis Machine (Luke Mcfarlan couldn’t make it) and “7 Lessons Learned Building Teams to Solve Problems Using ML” by Priscilla Alexander. These two presenters had similar talks, in which they both gave advice on how to effectively integrate data scientists and software engineers into work teams. 

Miriam Friedel’s presentation on Machine Learning explained their best practices for operationalizing data science and how to take machine learning models and ensure they deliver real business value. She explained how many businesses have tried utilizing Machine Learning without truly understanding the scientific backgrounds and operational challenges associated with doing it successfully. Building a machine learning model is iterative and requires a multi-faceted team. She used the analogy of not wanting to hire the engineer that built the microwave to run the whole kitchen. She also kept referring to the “Rickety Bridge Problem”: integrating ML into a complex infrastructure, particularly without a dedicated platform, results in disparate scripts, things cobbled together, and lots of humans in the loop. Additionally, only a small fraction of real-world ML systems is composed of the ML code, the required surrounding infrastructure is vast and complex. Businesses often focus solely on the machine learning algorithm and don’t put it in the appropriate context. One of her biggest messages was to get models into production as soon as possible to determine their return on investment. Additionally, she emphasized the importance of fully integrating the data scientists and software engineers into the business team because of the loss of scientific integrity associated with the “assembly line”. At Metis Machine they found that involving the data scientists in the product development from the start will make the entire team a lot more efficient because they can help put the algorithms into context. Overall she believes that failure with Machine Learning roots form organizational and technical issues. I am not positive where in her presentation this came up, but I really liked one of her quotes: “All models are wrong, but some are useful”. I think this goes back to a lot of the presenters’ points about needing to test models to know how good they are. In most real scenarios with ML, there is so much data that there will never be a correct or perfect model. As such, we must utilize the iterative process to determine the utility of our models. 

Priscilla Alexander was a former web app developer and now works for Capital One as a Machine Learning engineering director. Her presentation was centered around her 7 lessons for building Machine Learning solutions, which were explained in the context of business. The first lesson was to identify the problem before deciding Machine Learning is the solution. Is the problem deterministic? If so, it is easy to understand, rule-driven, and doesn’t need training data or special skills. Non-deterministic questions can be harder to explain and will require training data and specialized skills. Additionally, LOE is hard to predict, and may never work. Lesson 2 was to teach your business partners about Machine Learning and vice versa. Her third piece of advice was don’t be afraid to ask your data scientists dumb questions. She really tried to emphasize the importance of understanding the processes behind the algorithms in her second and third lessons. Both the previous presenter and her believed that the best teams have the data scientists and software engineers work very closely with the rest of the group. Lesson 4 was ‘it takes a village’. Lesson 5 was to get to the product as quickly as possible. This was some of the best advice from the event in my opinion. She really went on about how the only way to know how successful something will be is by trying it and to improve a product we must understand how it behaves via testing. I have personally noticed that when I started coding I wouldn’t be sure of the best way to go about programming something and would just overthink it too much. Overtime I started to realize that I was much more efficient when I would just go with my instinct and test/debug it. In engineering we emphasize the importance of the cycle of prototyping, testing, and updating the design requirements. In data science and computer science, however, we can’t usually predict how well something will work, especially when working with high-throughput data. Again, the best way to know how well a machine learning algorithm or model will predict your data is by testing it. Lesson 6 was if you don’t have the data, you don’t have the data. This is also an extremely important lesson in my opinion because it is imperative that as researchers we don’t try to force certain conclusions from their data. Ultimately, analyzing data inappropriately will produce irreproducible results. In computational biomedical engineering, the data we work with is extremely specific; attention to detail is crucial. We have to be extremely careful that we are measuring the right things, analyzing them appropriately, and making accurate conclusions. The last lesson was don’t ever stop learning. The field is always changing and if we aren’t learning and changing with it we are a huge disservice to ourselves and our fields. 
